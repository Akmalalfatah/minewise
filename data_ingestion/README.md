# ğŸ“Š Data Ingestion Layer - MineWise ML

## ğŸ“‹ Overview

**Data Ingestion & ETL Pipeline** untuk Mining Value Chain Optimization - sistem pengumpulan, transformasi, dan validasi data operasional tambang untuk machine learning pipeline.

**Dibuat oleh**: Tim Machine Learning - Data Engineering Team  
**Tujuan**: Data collection, cleaning, feature engineering, data quality validation  
**Status**: Production-Ready âœ…

---

## ğŸ¯ Tanggung Jawab Layer Ini

### 1. Data Collection ğŸ“¥
- Mengumpulkan data dari berbagai sumber (sensors, IoT devices, databases, APIs)
- Real-time data streaming dari operational systems
- Historical data extraction untuk model training

### 2. Data Cleaning ğŸ§¹
- Missing value handling (imputation, deletion)
- Outlier detection & treatment
- Data type consistency validation
- Duplicate removal

### 3. Data Transformation ğŸ”„
- Feature engineering dari raw data
- Normalization & standardization
- Encoding categorical variables
- Time-series aggregation

### 4. Data Validation âœ…
- Schema validation
- Data quality checks
- Referential integrity verification
- Business rule validation

### 5. Data Storage ğŸ’¾
- Raw data archival
- Processed data storage
- Data versioning & lineage tracking
- Metadata management

---

## ğŸ“‚ Struktur Folder

```
data_ingestion/
â”œâ”€â”€ raw/                    # Raw data dari sumber asli
â”‚   â”œâ”€â”€ sensors/           # Sensor data (CSV, JSON)
â”‚   â”œâ”€â”€ databases/         # Database dumps
â”‚   â”œâ”€â”€ apis/              # API responses
â”‚   â””â”€â”€ manual/            # Manual uploads
â”‚
â”œâ”€â”€ processed/             # Data yang sudah dibersihkan & ditransformasi
â”‚   â”œâ”€â”€ training/          # Data untuk model training
â”‚   â”œâ”€â”€ validation/        # Data untuk model validation
â”‚   â”œâ”€â”€ testing/           # Data untuk model testing
â”‚   â””â”€â”€ features/          # Engineered features
â”‚
â”œâ”€â”€ pipelines/             # ETL pipeline scripts
â”‚   â”œâ”€â”€ extraction/        # Data extraction scripts
â”‚   â”œâ”€â”€ transformation/    # Data transformation scripts
â”‚   â”œâ”€â”€ loading/           # Data loading scripts
â”‚   â””â”€â”€ orchestration/     # Airflow/Prefect DAGs
â”‚
â”œâ”€â”€ schema/                # Data schemas & validation rules
â”‚   â”œâ”€â”€ raw_schema.json    # Schema untuk raw data
â”‚   â”œâ”€â”€ processed_schema.json  # Schema untuk processed data
â”‚   â””â”€â”€ validation_rules.yaml  # Business rules
â”‚
â”œâ”€â”€ notebooks/             # Data exploration & analysis notebooks
â”‚   â”œâ”€â”€ eda/               # Exploratory Data Analysis
â”‚   â”œâ”€â”€ data_quality/      # Data quality reports
â”‚   â””â”€â”€ feature_engineering/  # Feature engineering experiments
â”‚
â”œâ”€â”€ configs/               # Configuration files
â”‚   â”œâ”€â”€ sources.yaml       # Data source configurations
â”‚   â”œâ”€â”€ pipeline.yaml      # Pipeline configurations
â”‚   â””â”€â”€ quality.yaml       # Data quality thresholds
â”‚
â””â”€â”€ README.md              # Documentation (this file)
```

---

## ğŸ”„ ETL Pipeline Workflow

### 1. Extraction Phase

```python
# Example: Extract data from PostgreSQL
from pipelines.extraction import DatabaseExtractor

extractor = DatabaseExtractor(
    host="production-db.example.com",
    database="mining_operations",
    table="vehicle_telemetry"
)

raw_data = extractor.extract(
    start_date="2025-01-01",
    end_date="2025-01-31"
)

raw_data.save("raw/databases/vehicle_telemetry_jan2025.csv")
```

### 2. Transformation Phase

```python
# Example: Clean & transform data
from pipelines.transformation import DataCleaner, FeatureEngineer

# Clean data
cleaner = DataCleaner()
cleaned_data = cleaner.clean(
    raw_data,
    drop_duplicates=True,
    handle_missing="impute",
    remove_outliers=True
)

# Engineer features
engineer = FeatureEngineer()
featured_data = engineer.transform(
    cleaned_data,
    features=["speed_variance", "load_efficiency", "weather_impact"]
)

featured_data.save("processed/features/vehicle_telemetry_features.csv")
```

### 3. Loading Phase

```python
# Example: Load to ML pipeline
from pipelines.loading import MLDataLoader

loader = MLDataLoader()
loader.load_training_data(
    features="processed/features/vehicle_telemetry_features.csv",
    target="road_speed",
    split_ratio=0.8
)
```

---

## ğŸ“Š Data Sources

### 1. Vehicle Telemetry
- **Source**: IoT sensors pada kendaraan tambang
- **Frequency**: Real-time (1Hz - 10Hz)
- **Fields**: speed, location, load, fuel_consumption, engine_temperature
- **Format**: JSON stream via MQTT

### 2. Weather Data
- **Source**: Weather API (OpenWeatherMap, BMKG)
- **Frequency**: Hourly
- **Fields**: temperature, humidity, wind_speed, precipitation, visibility
- **Format**: JSON via REST API

### 3. Road Condition Sensors
- **Source**: Fixed sensors sepanjang jalan tambang
- **Frequency**: Every 15 minutes
- **Fields**: surface_quality, dust_level, gradient, width
- **Format**: CSV via SFTP

### 4. Equipment Maintenance Logs
- **Source**: CMMS (Computerized Maintenance Management System)
- **Frequency**: Event-driven
- **Fields**: equipment_id, maintenance_type, downtime, cost, parts_replaced
- **Format**: PostgreSQL database

### 5. Production Data
- **Source**: Mine Planning System (Surpac, MineSched)
- **Frequency**: Daily
- **Fields**: material_type, tonnage, source_location, destination, cycle_time
- **Format**: CSV exports

### 6. Port Operations
- **Source**: Port Management System
- **Frequency**: Real-time
- **Fields**: vessel_arrival, loading_rate, berth_occupancy, tide_level
- **Format**: REST API

### 7. Fleet Management
- **Source**: Fleet Tracking System (GPS)
- **Frequency**: Real-time (5 second intervals)
- **Fields**: vehicle_id, location, speed, driver_id, status
- **Format**: JSON via WebSocket

---

## ğŸ§¹ Data Cleaning Strategies

### Missing Value Handling

| Data Type | Strategy | Reason |
|-----------|----------|--------|
| Numerical (continuous) | Mean/Median imputation | Preserve distribution |
| Numerical (categorical) | Mode imputation | Most frequent value |
| Time-series | Forward-fill | Temporal continuity |
| Critical fields | Drop row | Data integrity |

### Outlier Treatment

```python
# IQR Method
Q1 = df['speed'].quantile(0.25)
Q3 = df['speed'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

df_cleaned = df[(df['speed'] >= lower_bound) & (df['speed'] <= upper_bound)]
```

### Duplicate Handling

```python
# Remove exact duplicates
df_dedup = df.drop_duplicates()

# Remove duplicates based on key columns
df_dedup = df.drop_duplicates(subset=['vehicle_id', 'timestamp'])
```

---

## ğŸ”§ Feature Engineering

### Time-Based Features

```python
# Extract temporal features
df['hour'] = df['timestamp'].dt.hour
df['day_of_week'] = df['timestamp'].dt.dayofweek
df['is_night_shift'] = df['hour'].between(18, 6)
```

### Aggregation Features

```python
# Rolling window statistics
df['speed_rolling_mean'] = df.groupby('vehicle_id')['speed'].rolling(window=5).mean()
df['speed_rolling_std'] = df.groupby('vehicle_id')['speed'].rolling(window=5).std()
```

### Interaction Features

```python
# Create interaction terms
df['load_efficiency'] = df['actual_load'] / df['max_capacity']
df['speed_per_gradient'] = df['speed'] / (df['gradient'] + 1)  # +1 to avoid division by zero
```

### Encoding Categorical Variables

```python
# Label Encoding
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['weather_encoded'] = le.fit_transform(df['weather'])

# One-Hot Encoding
df = pd.get_dummies(df, columns=['vehicle_type'], prefix='vehicle')
```

---

## âœ… Data Validation

### Schema Validation

```yaml
# schema/raw_schema.json
{
  "vehicle_id": {
    "type": "string",
    "pattern": "^VH[0-9]{4}$",
    "required": true
  },
  "speed": {
    "type": "float",
    "min": 0,
    "max": 80,
    "required": true
  },
  "load": {
    "type": "float",
    "min": 0,
    "max": 100,
    "required": true
  },
  "timestamp": {
    "type": "datetime",
    "format": "ISO8601",
    "required": true
  }
}
```

### Data Quality Checks

```python
from pipelines.validation import DataQualityChecker

checker = DataQualityChecker()

# Check completeness
completeness = checker.check_completeness(df, threshold=0.95)

# Check uniqueness
uniqueness = checker.check_uniqueness(df, key_columns=['vehicle_id', 'timestamp'])

# Check consistency
consistency = checker.check_consistency(df, rules={
    'speed': lambda x: x >= 0,
    'load': lambda x: 0 <= x <= 100
})

# Generate report
checker.generate_report(output_file="data_quality_report.html")
```

---

## ğŸ“ˆ Data Quality Metrics

### Key Metrics

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| Completeness | â‰¥95% | 97.3% | âœ… |
| Uniqueness | 100% | 99.8% | âœ… |
| Timeliness | <5 min delay | 2 min | âœ… |
| Accuracy | â‰¥90% | 92.1% | âœ… |
| Consistency | â‰¥95% | 96.5% | âœ… |

### Monitoring Dashboard

```python
# Example: Track data quality over time
import plotly.express as px

quality_history = pd.read_csv("processed/quality_metrics_history.csv")
fig = px.line(quality_history, x='date', y=['completeness', 'accuracy', 'consistency'])
fig.show()
```

---

## ğŸš€ Running ETL Pipeline

### Manual Execution

```powershell
# Extract data
python pipelines/extraction/extract_vehicle_telemetry.py --date 2025-01-01

# Transform data
python pipelines/transformation/clean_and_transform.py --input raw/vehicle_telemetry_2025-01-01.csv

# Load data
python pipelines/loading/load_to_ml_pipeline.py --input processed/features/vehicle_telemetry_features.csv
```

### Automated Scheduling (Airflow)

```python
# pipelines/orchestration/daily_etl_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'ml-team',
    'retries': 3,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'daily_vehicle_telemetry_etl',
    default_args=default_args,
    schedule_interval='@daily',
    start_date=datetime(2025, 1, 1)
)

extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_vehicle_telemetry,
    dag=dag
)

transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=clean_and_transform,
    dag=dag
)

load_task = PythonOperator(
    task_id='load_data',
    python_callable=load_to_ml_pipeline,
    dag=dag
)

extract_task >> transform_task >> load_task
```

---

## ğŸ” Data Exploration Notebooks

### Available Notebooks

1. **[eda/vehicle_telemetry_eda.ipynb](notebooks/eda/vehicle_telemetry_eda.ipynb)**
   - Distribution analysis
   - Correlation analysis
   - Time-series visualization

2. **[data_quality/completeness_analysis.ipynb](notebooks/data_quality/completeness_analysis.ipynb)**
   - Missing value patterns
   - Data gap identification

3. **[feature_engineering/speed_prediction_features.ipynb](notebooks/feature_engineering/speed_prediction_features.ipynb)**
   - Feature importance analysis
   - Feature selection experiments

---

## ğŸ› Troubleshooting

### Common Issues

**1. Data Source Connection Timeout**
```powershell
# Check network connectivity
Test-Connection production-db.example.com

# Increase timeout in config
# configs/sources.yaml
connection_timeout: 60
```

**2. Memory Error During Processing**
```python
# Use chunking for large files
chunk_size = 10000
for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    process_chunk(chunk)
```

**3. Schema Validation Failed**
```powershell
# Validate schema before processing
python pipelines/validation/validate_schema.py --input raw/data.csv --schema schema/raw_schema.json
```

---

## ğŸ“ Support & Integration

### Integration with ML Pipeline

```python
# Machine Learning service dapat mengakses data via:
from machine_learning.src.data import DataLoader

loader = DataLoader()
train_data = loader.load_training_data(
    source="data_ingestion/processed/training/vehicle_telemetry.csv",
    features=['speed', 'load', 'weather', 'gradient'],
    target='cycle_time'
)
```

### Team Collaboration

- **Data Engineer**: Maintain ETL pipelines, ensure data quality
- **Data Scientist**: Explore data, engineer features, validate quality
- **ML Engineer**: Consume processed data, integrate with models

---

## ğŸ“ Changelog

### v1.0.0 (December 2025)
- âœ… Initial data ingestion layer setup
- âœ… ETL pipeline structure
- âœ… 7 data sources integration
- âœ… Data quality validation framework
- âœ… Feature engineering utilities

---

## ğŸ“ Credits

**Team**: Data Engineering Team - Machine Learning Capstone Project 2025  
**Focus**: Mining Operations Data Pipeline  
**Tech Stack**: Python, Pandas, SQLAlchemy, Apache Airflow  
**Year**: 2025

---

**Last Updated**: December 2025  
**Version**: 1.0.0  
**Status**: âœ… Production-Ready
