{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20315b27",
   "metadata": {},
   "source": [
    "# Data Pipeline & EDA: Mining Logistics Optimization\n",
    "\n",
    "**Role:** ML Engineer A  \n",
    "**Project:** Mining Value Chain - Logistics Simulation  \n",
    "**Tujuan:** Menyiapkan data operasional untuk pemodelan prediksi *Cycle Time* dan *Risiko Delay*.\n",
    "\n",
    "---\n",
    "\n",
    "### Deskripsi Tugas\n",
    "\n",
    "Notebook ini adalah tahap pertama (Notebook 1 dari 3) dalam pipeline Machine Learning. Fokus utama kita adalah:\n",
    "\n",
    "1. **Data Ingestion:** Memuat dataset operasional infrastruktur tambang (`ms_infra_transport_cycle_10k.csv`).\n",
    "2. **Data Splitting:** Memisahkan data menjadi dua aliran: dataset operasional (untuk Regresi) dan dataset lengkap termasuk breakdown (untuk Klasifikasi Risiko)\n",
    "3. **Data Saving:** Menyimpan data bersih dalam format `csv` untuk efisiensi di tahap modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb2e0d",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "Mengimpor library standar untuk manipulasi data tabular (`pandas`) dan visualisasi data (`matplotlib`, `seaborn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5665deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0d79d",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion (Pemuatan Data)\n",
    "\n",
    "Kita memuat dataset mentah. Langkah krusial di sini adalah memastikan kolom tanggal (`tanggal_operasi`) dibaca sebagai tipe data `datetime` agar urutan waktu (time-series) terjaga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6431061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil memuat ../../data/processed/infra_transport_cycle_10k.csv\n",
      "Tipe data tanggal berhasil diperbaiki\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "nama_file = \"../../data/processed/infra_transport_cycle_10k.csv\"\n",
    "\n",
    "try:\n",
    "    # Baca file CSV\n",
    "    df = pd.read_csv(nama_file)\n",
    "    print(f\"Berhasil memuat {nama_file}\")\n",
    "    \n",
    "    # Konversi kolom tanggal ke tipe datetime\n",
    "    df['tanggal_operasi'] = pd.to_datetime(df['tanggal_operasi'])\n",
    "    \n",
    "    # Urutkan data berdasarkan waktu (penting untuk time-series)\n",
    "    df = df.sort_values('tanggal_operasi')\n",
    "    \n",
    "    print(\"Tipe data tanggal berhasil diperbaiki\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File {nama_file} tidak ditemukan\")\n",
    "    print(\"Pastikan file csv berada di folder yang sama dengan notebook ini.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfb0ae9",
   "metadata": {},
   "source": [
    "## 3. Inspeksi Kesehatan Data (Data Health Check)\n",
    "\n",
    "Sebelum pemrosesan, kita melakukan *screening* awal untuk mendeteksi anomali:\n",
    "\n",
    "* **Tipe Data:** Memastikan angka dibaca sebagai numerik, bukan teks.\n",
    "* **Missing Values:** Mengidentifikasi kolom yang memiliki data kosong (Null/NaN).\n",
    "* **Statistik Deskriptif:** Melihat sebaran data target (`cycle_time_avg_jam`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b3a2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "INFO DATASET\n",
      "==================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10000 entries, 0 to 8791\n",
      "Data columns (total 32 columns):\n",
      " #   Column                  Non-Null Count  Dtype         \n",
      "---  ------                  --------------  -----         \n",
      " 0   id_record               10000 non-null  object        \n",
      " 1   id_alat                 10000 non-null  object        \n",
      " 2   tanggal_operasi         10000 non-null  datetime64[ns]\n",
      " 3   shift                   10000 non-null  object        \n",
      " 4   jam_mulai               10000 non-null  object        \n",
      " 5   jam_selesai             10000 non-null  object        \n",
      " 6   status_operasi          10000 non-null  object        \n",
      " 7   durasi_jam              10000 non-null  float64       \n",
      " 8   material_dipindah       10000 non-null  object        \n",
      " 9   total_muatan_ton        10000 non-null  float64       \n",
      " 10  jumlah_ritase           10000 non-null  int64         \n",
      " 11  id_operator             10000 non-null  object        \n",
      " 12  lokasi_kode             10000 non-null  object        \n",
      " 13  tipe_alat               10000 non-null  object        \n",
      " 14  model_alat              10000 non-null  object        \n",
      " 15  kapasitas_default_ton   10000 non-null  float64       \n",
      " 16  departemen              10000 non-null  object        \n",
      " 17  umur_tahun              10000 non-null  float64       \n",
      " 18  kondisi                 10000 non-null  object        \n",
      " 19  hujan_mm                10000 non-null  float64       \n",
      " 20  prob_hujan              10000 non-null  float64       \n",
      " 21  intensitas_hujan        10000 non-null  object        \n",
      " 22  suhu_min_c              10000 non-null  float64       \n",
      " 23  suhu_max_c              10000 non-null  float64       \n",
      " 24  kelembaban_rh_avg       10000 non-null  float64       \n",
      " 25  angin_kecepatan_avg_ms  10000 non-null  float64       \n",
      " 26  angin_gust_max_ms       10000 non-null  float64       \n",
      " 27  petir_count             10000 non-null  int64         \n",
      " 28  cycle_time_avg_jam      7427 non-null   float64       \n",
      " 29  ritase_per_jam          10000 non-null  float64       \n",
      " 30  ton_per_jam             10000 non-null  float64       \n",
      " 31  label_risk_delay        10000 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(14), int64(3), object(14)\n",
      "memory usage: 2.5+ MB\n",
      "\n",
      "==================================================\n",
      "MISSING VALUES\n",
      "==================================================\n",
      "cycle_time_avg_jam    2573\n",
      "dtype: int64\n",
      "\n",
      "==================================================\n",
      "STATISTIK CYCLE TIME\n",
      "==================================================\n",
      "count    7427.000000\n",
      "mean        0.598894\n",
      "std         0.352863\n",
      "min         0.167222\n",
      "25%         0.356154\n",
      "50%         0.485714\n",
      "75%         0.736000\n",
      "max         1.982500\n",
      "Name: cycle_time_avg_jam, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cek informasi dataset\n",
    "print(\"=\" * 50)\n",
    "print(\"INFO DATASET\")\n",
    "print(\"=\" * 50)\n",
    "df.info()\n",
    "\n",
    "# Cek missing values\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\" * 50)\n",
    "missing_data = df.isnull().sum()\n",
    "print(missing_data[missing_data > 0])  # Tampilkan hanya kolom dengan missing values\n",
    "\n",
    "# Statistik deskriptif target variable\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STATISTIK CYCLE TIME\")\n",
    "print(\"=\" * 50)\n",
    "print(df['cycle_time_avg_jam'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa9044d",
   "metadata": {},
   "source": [
    "## 4. Pembuatan Fitur Baru dan Split Logic\n",
    "\n",
    "Terdapat fitur-fitur yang akan menyebabkan data leakage dan jika dihapus, model tidak bisa melihat polanya sehingga pembuatan fitur baru ini penting untuk model melihat pola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e1f9c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitur lag berhasil ditambahkan!\n"
     ]
    }
   ],
   "source": [
    "# LAG FEATURES (Fitur Antrian)\n",
    "# Urutkan data berdasarkan lokasi dan waktu\n",
    "df = df.sort_values(by=['lokasi_kode', 'tanggal_operasi'])\n",
    "\n",
    "# Fitur 1: Cycle time unit sebelumnya di lokasi yang sama\n",
    "# Logika: Jika truk depan lambat, truk belakang akan ikut lambat (efek antrian)\n",
    "df['prev_cycle_time'] = df.groupby('lokasi_kode')['cycle_time_avg_jam'].shift(1)\n",
    "\n",
    "# Fitur 2: Rolling average dari 3 cycle terakhir\n",
    "# Logika: Rata-rata performa 3 truk terakhir di lokasi tersebut\n",
    "df['avg_last_3_cycles'] = df.groupby('lokasi_kode')['cycle_time_avg_jam'].transform(\n",
    "    lambda x: x.rolling(window=3).mean()\n",
    ")\n",
    "\n",
    "# Isi nilai NaN dengan backward fill (untuk data awal yang tidak punya history)\n",
    "df = df.bfill()\n",
    "\n",
    "print(\"Fitur lag berhasil ditambahkan!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f7c9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Regresi: 7427 baris\n",
      "Dataset Klasifikasi: 10000 baris\n"
     ]
    }
   ],
   "source": [
    "# SPLIT LOGIC - Memisahkan dataset untuk Regresi dan Klasifikasi\n",
    "\n",
    "# A. Dataset untuk REGRESI (Prediksi Cycle Time)\n",
    "# Syarat: Unit harus dalam status 'Beroperasi' dan Cycle Time valid (> 0)\n",
    "# Data Standby/Breakdown tidak masuk karena cycle time = 0 (tidak logis untuk regresi)\n",
    "df_reg = df[\n",
    "    (df['status_operasi'] == 'Beroperasi') &\n",
    "    (df['cycle_time_avg_jam'] > 0) &\n",
    "    (df['cycle_time_avg_jam'].notnull())\n",
    "].copy()\n",
    "\n",
    "# B. Dataset untuk KLASIFIKASI (Prediksi Risiko Delay/Breakdown)\n",
    "# Pakai semua data, termasuk Standby/Rusak sebagai contoh kelas \"Berisiko\"\n",
    "df_clf = df.copy()\n",
    "\n",
    "print(f\"Dataset Regresi: {len(df_reg)} baris\")\n",
    "print(f\"Dataset Klasifikasi: {len(df_clf)} baris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8032395a",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning and Engineering\n",
    "\n",
    "**Strategi:** Ditemukan adanya kolom yang tidak penting dan menyebabkan leakage sehingga harus dihapus. Selain itu, ekstraksi jam dan logika hujan sangat berpengaruh.\n",
    "\n",
    "**Tindakan:**\n",
    "* Ekstraksi waktu (jam operasi)\n",
    "* Thresholding logika hujan\n",
    "* Hapus data leakage\n",
    "* Handling identifiers\n",
    "\n",
    "**Alasan:** Waktu sangat berpengaruh untuk traffic tambang, menerapkan logika hujan untuk mengidentifikasi hujan lebat atau tidak, data leakage akan menyebabkan model gagal memahami pola, dan identifiers khususnya di sini (`id_record`) tidak berpengaruh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f838ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_engineer(dataframe):\n",
    "    \"\"\"\n",
    "    Membersihkan dan membuat fitur baru pada dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframe : pd.DataFrame\n",
    "        Dataset yang akan dibersihkan\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Dataset yang sudah dibersihkan\n",
    "    \"\"\"\n",
    "    # 1. Ekstraksi Waktu\n",
    "    # Jam operasi sangat mempengaruhi traffic tambang (shift pagi vs malam)\n",
    "    # Format jam_mulai 'HH:MM:SS', ambil HH saja\n",
    "    dataframe['jam_operasi'] = dataframe['jam_mulai'].str[:2].astype(int)\n",
    "    \n",
    "    # 2. Logika Hujan (Thresholding)\n",
    "    # Hujan > 10mm = jalan licin, risiko delay tinggi\n",
    "    dataframe['hujan_deras'] = dataframe['hujan_mm'].apply(lambda x: 1 if x > 10 else 0)\n",
    "    \n",
    "    # 3. Hapus Data Leakage\n",
    "    # Kolom ini adalah hasil perhitungan dari cycle_time, tidak boleh jadi input model\n",
    "    leakage_cols = ['ritase_per_jam', 'ton_per_jam', 'jumlah_ritase', \n",
    "                    'total_muatan_ton', 'durasi_jam']\n",
    "    dataframe = dataframe.drop(columns=leakage_cols, errors='ignore')\n",
    "    \n",
    "    # 4. Handling Identifiers\n",
    "    # Buang id_record (unik per baris, tidak ada pola)\n",
    "    # Simpan id_alat dan id_operator (akan di-encode di notebook berikutnya)\n",
    "    dataframe = dataframe.drop(columns=['id_record'], errors='ignore')\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "780ab782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Regresi (Unit Beroperasi): 7427 baris\n",
      "Data Klasifikasi (Semua Unit): 10000 baris\n",
      "Jumlah kolom: 30\n"
     ]
    }
   ],
   "source": [
    "# Terapkan cleaning ke kedua dataset\n",
    "df_reg_clean = clean_and_engineer(df_reg)\n",
    "df_clf_clean = clean_and_engineer(df_clf)\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(f\"\\nData Regresi (Unit Beroperasi): {len(df_reg_clean)} baris\")\n",
    "print(f\"Data Klasifikasi (Semua Unit): {len(df_clf_clean)} baris\")\n",
    "print(f\"Jumlah kolom: {len(df_reg_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e9e00",
   "metadata": {},
   "source": [
    "## 6. Penyimpanan Data (Data Saving)\n",
    "\n",
    "Menyimpan data yang sudah bersih (Clean Dataset) ke dalam format `.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step:**\n",
    "* `data_regression_clean.csv` akan digunakan pada **Notebook 02 (Modeling Regresi)**\n",
    "* `data_classification_clean.csv` akan digunakan pada **Notebook 03 (Modeling Klasifikasi)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b5ad565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DATA BERHASIL DISIMPAN\n",
      "==================================================\n",
      "1. data_regression_clean.csv -> Untuk Notebook 02 (Modeling Regresi)\n",
      "2. data_classification_clean.csv -> Untuk Notebook 03 (Modeling Klasifikasi)\n",
      "\n",
      "Notebook 01 selesai!\n"
     ]
    }
   ],
   "source": [
    "# Simpan dataset yang sudah dibersihkan\n",
    "df_reg_clean.to_csv(\"../../data/processed/data_regression_clean.csv\", index=False)\n",
    "df_clf_clean.to_csv(\"../../data/processed/data_classification_clean.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA BERHASIL DISIMPAN\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. data_regression_clean.csv -> Untuk Notebook 02 (Modeling Regresi)\")\n",
    "print(\"2. data_classification_clean.csv -> Untuk Notebook 03 (Modeling Klasifikasi)\")\n",
    "print(\"\\nNotebook 01 selesai!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
